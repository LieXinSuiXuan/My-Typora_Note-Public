[toc]



| 节点                                    | 主机名 | IP地址         |
| --------------------------------------- | ------ | -------------- |
| mycat，zookeeper1，kafka1，redis，nginx | mycat  | 192.168.20.11  |
| db1，zookeeper2，kafka2，jar1           | db1    | 192.168.200.12 |
| db2，zookeeper3，kafka3，jar2           | db2    | 192.168.200.13 |

# 一、构建读写分离的数据库集群

## 1、准备工作

### 1）配置计算机名

~~~powershell
[root@xonde1 ~]# hostnamectl set-hostname mycat
[root@xonde1 ~]# bash
[root@mycat ~]#

[root@xonde2 ~]# hostnamectl set-hostname db1
[root@xonde2 ~]# bash
[root@db1 ~]#

[root@xonde3 ~]# hostnamectl set-hostname db2
[root@xonde3 ~]# bash
[root@db2 ~]#
~~~



### 2）配置/etc/hosts文件

~~~powershell
[root@db1 ~]# vi /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.200.11  mycat
192.168.200.12  db1
192.168.200.13  db2
192.168.200.11  kafka1.mall
192.168.200.12  kafka1.mall
192.168.200.13  kafka1.mall
192.168.200.11  zk1.mall
192.168.200.12  zk2.mall
192.168.200.13  zk3.mall
192.168.200.11  redis.mall
192.168.200.11  mysql.mall				//mysql.mall即mycat节点

[root@mycat ~]# scp /etc/hosts db1:/etc/hosts
The authenticity of host 'db1 (192.168.200.12)' can't be established.
ECDSA key fingerprint is 10:a1:26:13:f5:8b:f7:71:a1:e5:e9:1f:7c:cf:dc:47.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'db1,192.168.200.12' (ECDSA) to the list of known hosts.
root@db1's password: 
hosts                                                                                                                                  100%  433     0.4KB/s   00:00    
[root@mycat ~]# scp /etc/hosts db2:/etc/hosts
The authenticity of host 'db2 (192.168.200.13)' can't be established.
ECDSA key fingerprint is 10:a1:26:13:f5:8b:f7:71:a1:e5:e9:1f:7c:cf:dc:47.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'db2,192.168.200.13' (ECDSA) to the list of known hosts.
root@db2's password: 
hosts                                                                                                                                  100%  433     0.4KB/s   00:00
~~~



### 3）3台主机均关闭防火墙和SELinux

~~~powershell
[root@mycat ~]# systemctl stop firewalld
[root@mycat ~]# systemctl disable firewalld
Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.
Removed symlink /etc/systemd/system/basic.target.wants/firewalld.service.
[root@mycat ~]# iptables -F
[root@mycat ~]# iptables -X
[root@mycat ~]# iptables -Z
[root@mycat ~]# iptables-save
# Generated by iptables-save v1.4.21 on Thu Nov 19 08:55:39 2020
*filter
:INPUT ACCEPT [14:1232]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [8:1088]
COMMIT
# Completed on Thu Nov 19 08:55:39 2020
[root@mycat ~]# setenforce 0
[root@mycat ~]# vi /etc/selinux/config			//此处编辑也可使用"sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config"代替

# This file controls the state of SELinux on the system.
# SELINUX= can take one of these three values:
#     enforcing - SELinux security policy is enforced.
#     permissive - SELinux prints warnings instead of enforcing.
#     disabled - No SELinux policy is loaded.
SELINUX=disabled														//此处修改为SELINUX=disabled
# SELINUXTYPE= can take one of three two values:
#     targeted - Targeted processes are protected,
#     minimum - Modification of targeted policy. Only selected processes are protected.
#     mls - Multi Level Security protection.
SELINUXTYPE=targeted
~~~

其他两台机器也一样进行设置。



### 4）yum源配置

#### a、mycat主机

~~~powershell
[root@mycat ~]# rm -f /etc/yum.repos.d/*
[root@mycat ~]# mkdir /opt/centos
[root@mycat ~]# mount CentOS-7-x86_64-DVD-1511.iso /opt/centos
mount: /dev/loop0 is write-protected, mounting read-only
[root@mycat ~]# mv gpmall-repo /opt/									//最好将gpmall-repo移出/root,否则另外两台主机使用ftp的yum源是会遇到权限的问题

[root@mycat ~]# vi /etc/yum.repos.d/local.repo
[centos]
name=cdrom
baseurl=file:///opt/centos
gpgcheck=0
enabled=1

[gpmall]
name=local
baseurl=file:///opt/gpmall-repo
gpgcheck=0
enabled=1

[root@mycat ~]# yum clean all
Loaded plugins: fastestmirror
Cleaning repos: centos gpmall
Cleaning up everything
Cleaning up list of fastest mirrors
[root@mycat ~]# yum repolist
Loaded plugins: fastestmirror
centos                                                                                                                                                                                                                             | 3.6 kB  00:00:00     
gpmall                                                                                                                                                                                                                             | 2.9 kB  00:00:00     
(1/3): centos/primary_db                                                                                                                                                                                                           | 2.8 MB  00:00:00     
(2/3): centos/group_gz                                                                                                                                                                                                             | 155 kB  00:00:00     
(3/3): gpmall/primary_db                                                                                                                                                                                                           | 144 kB  00:00:00     
Determining fastest mirrors
repo id                                                                                                                    repo name                                                                                                                status
centos                                                                                                                     cdrom                                                                                                                    3,723
gpmall                                                                                                                     local                                                                                                                      165
repolist: 3,888

[root@mycat ~]# yum install -y vsftpd
[root@mycat ~]# vi /etc/vsftpd/vsftpd.conf
anon_root=/opt															//在文件最开始添加此句，将/opt设置为匿名用户访问

[root@mycat ~]# systemctl start vsftpd
[root@mycat ~]# systemctl enable vsftpd
Created symlink from /etc/systemd/system/multi-user.target.wants/vsftpd.service to /usr/lib/systemd/system/vsftpd.service.

~~~

#### b、另两台主机配置yum源

~~~powershell
[root@db1 ~]# rm -f /etc/yum.repos.d/*

[root@mycat ~]# vi /etc/yum.repos.d/local.repo
[centos]
name=ftp
baseurl=ftp://192.168.200.11/centos
gpgcheck=0
enabled=1

[gpmall]
name=ftp
baseurl=ftp://192.168.200.11/gpmall-repo
gpgcheck=0
enabled=1

[root@db1 ~]# yum clean all
Loaded plugins: fastestmirror
Cleaning repos: centos gpmall
Cleaning up everything
[root@db1 yum.repos.d]# yum repolist
Loaded plugins: fastestmirror
centos                                                                                                                                                                                                                             | 3.6 kB  00:00:00     
gpmall                                                                                                                                                                                                                             | 2.9 kB  00:00:00     
(1/3): centos/group_gz                                                                                                                                                                                                             | 155 kB  00:00:00     
(2/3): centos/primary_db                                                                                                                                                                                                           | 2.8 MB  00:00:00     
(3/3): gpmall/primary_db                                                                                                                                                                                                           | 144 kB  00:00:00     
Determining fastest mirrors
repo id                                                                                                                  repo name                                                                                                                  status
centos                                                                                                                   ftp                                                                                                                        3,723
gpmall                                                                                                                   ftp                                                                                                                    165
repolist: 3,888

#db2的yum配置文件可以使用scp传输过去，但还是运行yum clean all和yum repolist

[root@db1 ~]# scp /etc/yum.repos.d/local.repo db2:/etc/yum.repos.d/
The authenticity of host 'db2 (192.168.200.13)' can't be established.
ECDSA key fingerprint is 10:a1:26:13:f5:8b:f7:71:a1:e5:e9:1f:7c:cf:dc:47.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'db2,192.168.200.13' (ECDSA) to the list of known hosts.
root@db2's password: 
local.repo                                                                                                                             100%  156     0.2KB/s   00:00    

[root@db2 ~]# yum clean all
Loaded plugins: fastestmirror
Cleaning repos: centos gpmall
Cleaning up everything
[root@db2 ~]# yum repolist
Loaded plugins: fastestmirror
centos                                                                                                                                            | 3.6 kB  00:00:00     
gpmall                                                                                                                                            | 2.9 kB  00:00:00     
(1/3): centos/group_gz                                                                                                                            | 155 kB  00:00:00     
(2/3): centos/primary_db                                                                                                                          | 2.8 MB  00:00:00     
(3/3): gpmall/primary_db                                                                                                                          | 144 kB  00:00:00     
Determining fastest mirrors
repo id                                                                            repo name                                                                       status
centos                                                                             ftp                                                                             3,723
gpmall                                                                             ftp                                                                               165
repolist: 3,888
~~~

### 5）三个节点安装JDK环境（Mycat节点在安装mycat时需要JDK环境，db1和db2在安装zookeeper节点是也需要使用到JDK环境）

~~~powershell
[root@mycat ~]# yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-devel

[root@mycat ~]# java -version
openjdk version "1.8.0_222"
OpenJDK Runtime Environment (build 1.8.0_222-b10)
OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode)
~~~



## 2、部署MariaDB主从数据库集群服务

### 1）在db1和db2两个节点安装MariaDB服务，并启用

~~~powershell
[root@db1 ~]# yum install -y mariadb mariadb-server

[root@db1 ~]# systemctl start mariadb
[root@db1 ~]# systemctl enable mariadb
[root@db1 ~]# mysql_secure_installation

NOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MariaDB
      SERVERS IN PRODUCTION USE!  PLEASE READ EACH STEP CAREFULLY!

In order to log into MariaDB to secure it, we'll need the current
password for the root user.  If you've just installed MariaDB, and
you haven't set the root password yet, the password will be blank,
so you should just press enter here.

Enter current password for root (enter for none): 
OK, successfully used password, moving on...

Setting the root password ensures that nobody can log into the MariaDB
root user without the proper authorisation.

Set root password? [Y/n] y
New password: 						//输入密码123456
Re-enter new password: 				//再次输入密码123456
Password updated successfully!
Reloading privilege tables..
 ... Success!


By default, a MariaDB installation has an anonymous user, allowing anyone
to log into MariaDB without having to have a user account created for
them.  This is intended only for testing, and to make the installation
go a bit smoother.  You should remove them before moving into a
production environment.

Remove anonymous users? [Y/n] y
 ... Success!

Normally, root should only be allowed to connect from 'localhost'.  This
ensures that someone cannot guess at the root password from the network.

Disallow root login remotely? [Y/n] n
 ... skipping.

By default, MariaDB comes with a database named 'test' that anyone can
access.  This is also intended only for testing, and should be removed
before moving into a production environment.

Remove test database and access to it? [Y/n] y
 - Dropping test database...
 ... Success!
 - Removing privileges on test database...
 ... Success!

Reloading the privilege tables will ensure that all changes made so far
will take effect immediately.

Reload privilege tables now? [Y/n] y
 ... Success!

Cleaning up...

All done!  If you've completed all of the above steps, your MariaDB
installation should now be secure.

Thanks for using MariaDB!
~~~

### 2）配置数据库集群主节点

~~~powershell
[root@db1 ~]# vi /etc/my.cnf
//添加以下的信息
[mysqld]
log_bin = mysql-bin 
binlog_ignore_db = mysql
server_id = 12
#
# This group is read both both by the client and the server
# use it for options that affect everything
#
[client-server]

#
# include all files from the config directory
#
!includedir /etc/my.cnf.d


[root@db1 ~]# systemctl restart mariadb
~~~

### 3）开放主节点的数据库权限

~~~powershell
[root@db1 ~]# mysql -uroot -p123456
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 9
Server version: 10.3.18-MariaDB-log MariaDB Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]> grant all privileges on *.* to root@'%' identified by "123456";		//授权在任何客户端机器上可以以root用户登录到数据库
Query OK, 0 rows affected (0.001 sec)

MariaDB [(none)]> grant replication slave on *.* to 'user'@'db2' identified by '123456';	//创建一个user用户让从节点db2连接，并赋予从节点同步主节点数据库的权限
Query OK, 0 rows affected (0.000 sec)

MariaDB [(none)]> exit
Bye
~~~

### 4）配置从节点db2同步主节点db1

~~~powershell
[root@db2 ~]# mysql -uroot -p123456
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 15
Server version: 10.3.18-MariaDB MariaDB Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]> change master to master_host='db1',master_user='user',master_password='123456';	//配置从节点连接主节点的连接信息。
master_host为主节点主机名db1，master_user为在db1中创建的用户user
Query OK, 0 rows affected (0.008 sec)

MariaDB [(none)]> start slave;
Query OK, 0 rows affected (0.006 sec)

MariaDB [(none)]> show slave status\G;
*************************** 1. row ***************************
                Slave_IO_State: Waiting for master to send event
                   Master_Host: db1
                   Master_User: user
                   Master_Port: 3306
                 Connect_Retry: 60
               Master_Log_File: mysql-bin.000001
           Read_Master_Log_Pos: 701
                Relay_Log_File: db2-relay-bin.000002
                 Relay_Log_Pos: 1000
         Relay_Master_Log_File: mysql-bin.000001
              Slave_IO_Running: Yes						//此处为Yes
             Slave_SQL_Running: Yes						//此处为Yes，这两处均为Yes，则代表从节点服务开启成功
               Replicate_Do_DB: 
           Replicate_Ignore_DB: 
            Replicate_Do_Table: 
        Replicate_Ignore_Table: 
       Replicate_Wild_Do_Table: 
   Replicate_Wild_Ignore_Table: 
                    Last_Errno: 0
                    Last_Error: 
                  Skip_Counter: 0
           Exec_Master_Log_Pos: 701
               Relay_Log_Space: 1307
               Until_Condition: None
                Until_Log_File: 
                 Until_Log_Pos: 0
            Master_SSL_Allowed: No
            Master_SSL_CA_File: 
            Master_SSL_CA_Path: 
               Master_SSL_Cert: 
             Master_SSL_Cipher: 
                Master_SSL_Key: 
         Seconds_Behind_Master: 0
 Master_SSL_Verify_Server_Cert: No
                 Last_IO_Errno: 0
                 Last_IO_Error: 
                Last_SQL_Errno: 0
                Last_SQL_Error: 
   Replicate_Ignore_Server_Ids: 
              Master_Server_Id: 18
                Master_SSL_Crl: 
            Master_SSL_Crlpath: 
                    Using_Gtid: No
                   Gtid_IO_Pos: 
       Replicate_Do_Domain_Ids: 
   Replicate_Ignore_Domain_Ids: 
                 Parallel_Mode: conservative
                     SQL_Delay: 0
           SQL_Remaining_Delay: NULL
       Slave_SQL_Running_State: Slave has read all relay log; waiting for the slave I/O thread to update it
              Slave_DDL_Groups: 2
Slave_Non_Transactional_Groups: 0
    Slave_Transactional_Groups: 0
1 row in set (0.000 sec)
~~~



## 3、部署Mycat读写分离中间件服务



### 1）安装Mycat

~~~powershell
[root@mycat ~]# tar zxvf Mycat-server-1.6-RELEASE-20161028204710-linux.tar.gz -C /usr/local/
[root@mycat ~]# chmod -R 777 /usr/local/mycat/
[root@mycat ~]# echo export MYCAT_HOME=/usr/local/mycat/ >> /etc/profile
[root@mycat ~]# source /etc/profile
~~~



### 2）编辑Mycat的逻辑库配置文件

~~~powershell
[root@mycat ~]# rm -f /usr/local/mycat/conf/schema.xml
[root@mycat ~]# vi /usr/local/mycat/conf/schema.xml
<?xml version="1.0"?>
<!DOCTYPE mycat:schema SYSTEM "schema.dtd">
<mycat:schema xmlns:mycat="http://io.mycat/">

        <schema name="USERDB" checkSQLschema="true" sqlMaxLimit="100" dataNode="dn1"></schema>
        <dataNode name="dn1" dataHost="localhost1" database="test" />
        <dataHost name="localhost1" maxCon="1000" minCon="10" balance="0" writeType="0" dbType="mysql" dbDriver="native" switchType="1"  slaveThreshold="100">
                <heartbeat>select user()</heartbeat>
                <writeHost host="hostM1" url="192.168.200.12:3306" user="root" password="123456">
                        <readHost host="hostS1" url="192.168.200.13:3306" user="root" password="123456" />
                </writeHost>
        </dataHost>
</mycat:schema>
~~~



### 3）修改配置文件权限

~~~powershell
[root@mycat ~]# chown root:root /usr/local/mycat/conf/schema.xml
~~~



### 4）编辑mycat的访问用户

~~~powershell
[root@mycat ~]# vi /usr/local/mycat/conf/server.xml
<?xml version="1.0" encoding="UTF-8"?>
<!-- - - Licensed under the Apache License, Version 2.0 (the "License");
        - you may not use this file except in compliance with the License. - You
        may obtain a copy of the License at - - http://www.apache.org/licenses/LICENSE-2.0
        - - Unless required by applicable law or agreed to in writing, software -
        distributed under the License is distributed on an "AS IS" BASIS, - WITHOUT
        WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. - See the
        License for the specific language governing permissions and - limitations
        under the License. -->
<!DOCTYPE mycat:server SYSTEM "server.dtd">
<mycat:server xmlns:mycat="http://io.mycat/">
        <system>
        <property name="useSqlStat">0</property>
        <property name="useGlobleTableCheck">0</property>

                <property name="sequnceHandlerType">2</property>
      <!--  <property name="useCompression">1</property>-->
        <!--  <property name="fakeMySQLVersion">5.6.20</property>-->
        <!-- <property name="processorBufferChunk">40960</property> -->
        <!--
        <property name="processors">1</property>
        <property name="processorExecutor">32</property>
         -->
                <!--默认为type 0: DirectByteBufferPool | type 1 ByteBufferArena-->
                <property name="processorBufferPoolType">0</property>
                <!--<property name="maxStringLiteralLength">65535</property>-->
                <!--<property name="sequnceHandlerType">0</property>-->
                <!--<property name="backSocketNoDelay">1</property>-->
                <!--<property name="frontSocketNoDelay">1</property>-->
                <!--<property name="processorExecutor">16</property>-->
                <!--
                        <property name="serverPort">8066</property> <property name="managerPort">9066</property>
                        <property name="idleTimeout">300000</property> <property name="bindIp">0.0.0.0</property>
                        <property name="frontWriteQueueSize">4096</property> <property name="processors">32</property> -->
                <property name="handleDistributedTransactions">0</property>
                <property name="useOffHeapForMerge">1</property>


                <property name="memoryPageSize">1m</property>
        <user name="user">
                <property name="password">user</property>
                <property name="spillsFileBufferSize">1k</property>
                <property name="useStreamOutput">0</property>
                <property name="systemReserveMemorySize">384m</property>
                <property name="useZKSwitch">true</property>


        </system>
        <!--
        <firewall>
           <whitehost>
              <host host="127.0.0.1" user="mycat"/>
              <host host="127.0.0.2" user="mycat"/>
           </whitehost>
       <blacklist check="false">
       </blacklist>
        </firewall>
        -->

        <user name="root">
                <property name="password">123456</property>
                <property name="schemas">USERDB</property>		//将此处原为TESTDB修改为USERDB
                <!--
                <privileges check="false">
                        <schema name="TESTDB" dml="0110" >						
                                <table name="tb01" dml="0000"></table>
                                <table name="tb02" dml="1111"></table>
                        </schema>
                </privileges>
                 -->
        </user>

        <user name="user">														//将此处开始的5行删除，开始删除
                <property name="password">user</property>
                <property name="schemas">TESTDB</property>
                <property name="readOnly">true</property>
        </user>

</mycat:server>
~~~



### 5）启动Mycat服务

~~~powershell
[root@mycat ~]# /bin/bash /usr/local/mycat/bin/mycat start
~~~



### 6）验证数据库集群服务读写分离功能

端口测试

~~~powershell
[root@mycat ~]# yum install -y net-tools									//xonde1中没有netstat工具，需要先进行安装

[root@mycat ~]# netstat -ntpl
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1466/sshd           
tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      1972/master         
tcp        0      0 127.0.0.1:32000         0.0.0.0:*               LISTEN      3195/java           
tcp6       0      0 :::9066                 :::*                    LISTEN      3195/java           
tcp6       0      0 :::47211                :::*                    LISTEN      3195/java           
tcp6       0      0 :::21                   :::*                    LISTEN      2875/vsftpd         
tcp6       0      0 :::53654                :::*                    LISTEN      3195/java           
tcp6       0      0 :::22                   :::*                    LISTEN      1466/sshd           
tcp6       0      0 ::1:25                  :::*                    LISTEN      1972/master         
tcp6       0      0 :::1984                 :::*                    LISTEN      3195/java           
tcp6       0      0 :::8066                 :::*                    LISTEN      3195/java           //8066和9066端口开放即表示正确
~~~

数据库测试

~~~powershell
[root@mycat ~]# yum install -y MariaDB-client		//先在mycat虚拟机上使用yum安装mariadb-client服务

[root@mycat ~]# mysql -h127.0.0.1 -P9066 -uroot -p123456 -e 'show @@datasource;'
+----------+--------+-------+----------------+------+------+--------+------+------+---------+-----------+------------+
| DATANODE | NAME   | TYPE  | HOST           | PORT | W/R  | ACTIVE | IDLE | SIZE | EXECUTE | READ_LOAD | WRITE_LOAD |
+----------+--------+-------+----------------+------+------+--------+------+------+---------+-----------+------------+
| dn1      | hostM1 | mysql | 192.168.200.12 | 3306 | W    |      0 |   10 | 1000 |     104 |         0 |          1 |
| dn1      | hostS1 | mysql | 192.168.200.13 | 3306 | R    |      0 |    8 | 1000 |     103 |         3 |          0 |
+----------+--------+-------+----------------+------+------+--------+------+------+---------+-----------+------------+
~~~



# 二、ZooKeeper分布式应用程序协调服务部署



## 1、解压ZooKeeper软件包

将zookeeper-3.4.14.tar.gz软件包上传至3个节点的/root目录下，进行解压操作，3个节点均执行操作。

~~~powershell
[root@mycat ~]# scp zookeeper-3.4.14.tar.gz db1:/root/
root@db1's password: 
zookeeper-3.4.14.tar.gz                                                                                                                                                                                                 100%   36MB  35.9MB/s   00:00
[root@mycat ~]# scp zookeeper-3.4.14.tar.gz db2:/root/
root@db2's password: zookeeper-3.4.14.tar.gz                                                                                                                                                                                                 100%   36MB  35.9MB/s   00:00
[root@mycat ~]# tar zxvf zookeeper-3.4.14.tar.gz

[root@db1 ~]# tar zxvf zookeeper-3.4.14.tar.gz

[root@db2 ~]# tar zxvf zookeeper-3.4.14.tar.gz
~~~



## 2、修改3个节点配置文件

~~~powershell
[root@mycat ~]# mv zookeeper-3.4.14/conf/zoo_sample.cfg zookeeper-3.4.14/conf/zoo.cfg
[root@mycat ~]# vi zookeeper-3.4.14/conf/zoo.cfg
# The number of milliseconds of each tick
tickTime=2000
# The number of ticks that the initial
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between
# sending a request and getting an acknowledgement
syncLimit=5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just
# example sakes.
dataDir=/tmp/zookeeper
# the port at which the clients will connect
clientPort=2181
# the maximum number of client connections.
# increase this if you need to handle more clients
#maxClientCnxns=60
#
# Be sure to read the maintenance section of the
# administrator guide before turning on autopurge.
#
# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
#
# The number of snapshots to retain in dataDir
#autopurge.snapRetainCount=3
# Purge task interval in hours
# Set to "0" to disable auto purge feature
#autopurge.purgeInterval=1
server.1=192.168.200.11:2888:3888			//添加此处开始的3行
server.2=192.168.200.12:2888:3888
server.3=192.168.200.13:2888:3888
~~~

db1和db2也是同样进行操作，使用scp同步到另外两台计算机也可。



## 3、创建myid文件

~~~powershell
[root@mycat ~]# mkdir /tmp/zookeeper
[root@mycat ~]# vi /tmp/zookeeper/myid
1

[root@db1 ~]# mkdir /tmp/zookeeper
[root@db1 ~]# vi /tmp/zookeeper/myid
2

[root@db2 ~]# mkdir /tmp/zookeeper
[root@db2 ~]# vi /tmp/zookeeper/myid
3
~~~



## 4、启动ZooKeeper服务

~~~powershell
[root@mycat ~]# zookeeper-3.4.14/bin/zkServer.sh start

[root@mycat ~]# zookeeper-3.4.14/bin/zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /root/zookeeper-3.4.14/bin/../conf/zoo.cfg
Mode: follower

[root@db1 ~]# zookeeper-3.4.14/bin/zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /root/zookeeper-3.4.14/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
[root@db1 ~]# zookeeper-3.4.14/bin/zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /root/zookeeper-3.4.14/bin/../conf/zoo.cfg
Mode: leader

[root@db2 ~]# zookeeper-3.4.14/bin/zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /root/zookeeper-3.4.14/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
[root@db2 ~]# zookeeper-3.4.14/bin/zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /root/zookeeper-3.4.14/bin/../conf/zoo.cfg
Mode: follower
~~~

3个节点都执行完zkServer.sh start之后，再执行zkServer.sh status进行状态查询。



# 三、Kafka分布式发布订阅消息系统部署



## 1、解压Kafka软件包

将kafka_2.11-1.1.1.tgz软件包上传至3个节点的/root目录下，进行解压操作，3个节点均执行操作。

~~~powershell
[root@mycat ~]# scp kafka_2.11-1.1.1.tgz db1:/root/
root@db1's password: 
kafka_2.11-1.1.1.tgz                                                                                                                                                                                                    100%   55MB  54.8MB/s   00:01     
[root@mycat ~]# scp kafka_2.11-1.1.1.tgz db2:/root/
root@db2's password: 
kafka_2.11-1.1.1.tgz                                                                                                                                                                                                    100%   55MB  54.8MB/s   00:01    
[root@mycat ~]# tar zxvf kafka_2.11-1.1.1.tgz

[root@db1 ~]# tar zxvf kafka_2.11-1.1.1.tgz

[root@db2 ~]# tar zxvf kafka_2.11-1.1.1.tgz
~~~



## 2、修改3个节点配置文件

~~~powershell
[root@mycat ~]# vi kafka_2.11-1.1.1/config/server.properties 
broker.id=1
zookeeper.connect=192.168.200.11:2181,192.168.200.12:2181,192.168.200.13:2181
//并添加如下一条信息
listeners = PLAINTEXT://192.168.200.11:9092


//kafka1修改完毕之后，可以使用scp传递到kafka2和kafka3两台计算机之后，再进行适当修改。

[root@db1 ~]# vi kafka_2.11-1.1.1/config/server.properties 
broker.id=2
zookeeper.connect=192.168.200.11:2181,192.168.200.12:2181,192.168.200.13:2181
listeners = PLAINTEXT://192.168.200.12:9092

[root@db2 ~]# vi kafka_2.11-1.1.1/config/server.properties 
broker.id=3
zookeeper.connect=192.168.200.11:2181,192.168.200.12:2181,192.168.200.13:2181
listeners = PLAINTEXT://192.168.200.13:9092
~~~

zookeeper.connect的配置每个节点是一样的，但是broker.id和listeners不同。



## 3、启动服务

3个节点的启动命令相同，返回的信息也基本相同。

~~~powershell
[root@mycat ~]# kafka_2.11-1.1.1/bin/kafka-server-start.sh -daemon kafka_2.11-1.1.1/config/server.properties

[root@mycat ~]# jps
6211 QuorumPeerMain
6645 Kafka
3195 WrapperSimpleApp
6719 Jps

[root@mycat ~]# netstat -ntpl
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1468/sshd           
tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      1910/master         
tcp        0      0 127.0.0.1:32000         0.0.0.0:*               LISTEN      3854/java           
tcp6       0      0 :::49320                :::*                    LISTEN      3854/java           
tcp6       0      0 :::9066                 :::*                    LISTEN      3854/java           
tcp6       0      0 :::36144                :::*                    LISTEN      4301/java           
tcp6       0      0 192.168.200.11:3888     :::*                    LISTEN      3954/java           
tcp6       0      0 :::38576                :::*                    LISTEN      3854/java           
tcp6       0      0 :::21                   :::*                    LISTEN      2877/vsftpd         
tcp6       0      0 :::22                   :::*                    LISTEN      1468/sshd           
tcp6       0      0 ::1:25                  :::*                    LISTEN      1910/master         
tcp6       0      0 :::35613                :::*                    LISTEN      3954/java           
tcp6       0      0 :::1984                 :::*                    LISTEN      3854/java           
tcp6       0      0 :::8066                 :::*                    LISTEN      3854/java           
tcp6       0      0 192.168.200.11:9092     :::*                    LISTEN      4301/java           
tcp6       0      0 :::2181                 :::*                    LISTEN      3954/java           
~~~



## 4、测试

在kafka1上生成测试消息，然后在2号和3号机上能够收到发送的信息。

~~~powershell
[root@mycat ~]# kafka_2.11-1.1.1/bin/kafka-topics.sh --create --zookeeper 192.168.200.11:2181 --replication-factor 1 --partitions 1 --topic test
Created topic "test".
~~~

在2号和3号机上接收。

~~~powershell
[root@db1 ~]# kafka_2.11-1.1.1/bin/kafka-topics.sh --list --zookeeper 192.168.200.12:2181
OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N
test

[root@db2 ~]# kafka_2.11-1.1.1/bin/kafka-topics.sh --list --zookeeper 192.168.200.13:2181
OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N
test
~~~



# 四、Redis应用系统消息队列服务



## 1、修改数据库配置

将提供的gpmall-cluster文件夹上传至数据库db1和db2节点的/root目录下。登录db1节点，使用数据库用户名为root，密码为123456登陆数据库，创建库gpmall，将其中的gpmall.sql文件到入到gpmall库中。

~~~powershell
[root@mycat ~]# scp -r gpmall-cluster/ db1:/root

[root@mycat ~]# scp -r gpmall-cluster/ db2:/root
~~~

然后在db1节点上导入gpmall.sql

~~~powershell
[root@db1 ~]# mysql -uroot -p123456
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 21
Server version: 10.3.18-MariaDB-log MariaDB Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]> create database gpmall;		//创建gpmall数据库
Query OK, 1 row affected (0.002 sec)

MariaDB [(none)]> use gpmall
Database changed
MariaDB [gpmall]> source /root/gpmall.sql		//加载gpmall.sql

........................
Query OK, 0 rows affected (0.001 sec)

MariaDB [gpmall]> exit
Bye
[root@db1 ~]# 
~~~



## 2、修改Mycat配置

### 1）修改schema.xml配置文件

将schema name修改为 gpmall，checkSQLschema修改为 false，database修改为gpmall。

~~~powershell
[root@mycat ~]# vi /usr/local/mycat/conf/schema.xml 
<?xml version="1.0"?>
<!DOCTYPE mycat:schema SYSTEM "schema.dtd">
<mycat:schema xmlns:mycat="http://io.mycat/">

        <schema name="gpmall" checkSQLschema="fales" sqlMaxLimit="100" dataNode="dn1"></schema>
        <dataNode name="dn1" dataHost="localhost1" database="gpmall" />
        <dataHost name="localhost1" maxCon="1000" minCon="10" balance="0" writeType="0" dbType="mysql" dbDriver="native" switchType="1"  slaveThreshold="100">
                <heartbeat>select user()</heartbeat>
                <writeHost host="hostM1" url="192.168.200.12:3306" user="root" password="123456">
                        <readHost host="hostS1" url="192.168.200.13:3306" user="root" password="123456" />
                </writeHost>
        </dataHost>
</mycat:schema>
~~~

### 2）修改server.xml配置文件

将<property name="schemas">USERDB</property>中的USERDB修改为gpmall

~~~powershell
[root@mycat ~]# vi /usr/local/mycat/conf/server.xml
.........................
        <user name="root">
                <property name="password">123456</property>
                <property name="schemas">gpmall</property>			//将此处原为USERDB修改为gpmall

                <!-- 表级 DML 权限设置 -->
                <!--
                <privileges check="false">
                        <schema name="TESTDB" dml="0110" >
                                <table name="tb01" dml="0000"></table>
                                <table name="tb02" dml="1111"></table>
                        </schema>
                </privileges>
                 -->
        </user>


</mycat:server>
~~~

### 3）重启Mycat服务

~~~powershell
[root@mycat ~]# /usr/local/mycat/bin/mycat restart
Stopping Mycat-server...
Stopped Mycat-server.
Starting Mycat-server...

[root@mycat ~]# netstat -ntpl
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1466/sshd           
tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      1972/master         
tcp        0      0 127.0.0.1:32000         0.0.0.0:*               LISTEN      7115/java           
tcp6       0      0 :::9066                 :::*                    LISTEN      7115/java           
tcp6       0      0 :::41835                :::*                    LISTEN      7115/java           
tcp6       0      0 :::60172                :::*                    LISTEN      7115/java           
tcp6       0      0 :::35663                :::*                    LISTEN      6645/java           
tcp6       0      0 192.168.200.11:3888     :::*                    LISTEN      6211/java           
tcp6       0      0 :::21                   :::*                    LISTEN      2875/vsftpd         
tcp6       0      0 :::22                   :::*                    LISTEN      1466/sshd           
tcp6       0      0 :::38232                :::*                    LISTEN      6211/java           
tcp6       0      0 ::1:25                  :::*                    LISTEN      1972/master         
tcp6       0      0 :::1984                 :::*                    LISTEN      7115/java           
tcp6       0      0 :::8066                 :::*                    LISTEN      7115/java           
tcp6       0      0 192.168.200.11:9092     :::*                    LISTEN      6645/java           
tcp6       0      0 :::2181                 :::*                    LISTEN      6211/java           
~~~



## 3、安装Redis服务

~~~powershell
[root@mycat ~]# yum install -y redis

[root@mycat ~]# vi /etc/redis.conf 

# bind 127.0.0.1				//将此句注释

protected-mode no				//将此句由yes改为no

[root@mycat ~]# systemctl start redis
[root@mycat ~]# systemctl enable redis
Created symlink from /etc/systemd/system/multi-user.target.wants/redis.service to /usr/lib/systemd/system/redis.service.

[root@mycat ~]# netstat -ntpl
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
tcp        0      0 0.0.0.0:6379            0.0.0.0:*               LISTEN      7217/redis-server * 	//检查到6379端口即可
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1466/sshd           
tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      1972/master         
tcp        0      0 127.0.0.1:32000         0.0.0.0:*               LISTEN      7115/java           
tcp6       0      0 :::9066                 :::*                    LISTEN      7115/java           
tcp6       0      0 :::6379                 :::*                    LISTEN      7217/redis-server * 
tcp6       0      0 :::41835                :::*                    LISTEN      7115/java           
tcp6       0      0 :::60172                :::*                    LISTEN      7115/java           
tcp6       0      0 :::35663                :::*                    LISTEN      6645/java           
tcp6       0      0 192.168.200.11:3888     :::*                    LISTEN      6211/java           
tcp6       0      0 :::21                   :::*                    LISTEN      2875/vsftpd         
tcp6       0      0 :::22                   :::*                    LISTEN      1466/sshd           
tcp6       0      0 :::38232                :::*                    LISTEN      6211/java           
tcp6       0      0 ::1:25                  :::*                    LISTEN      1972/master         
tcp6       0      0 :::1984                 :::*                    LISTEN      7115/java           
tcp6       0      0 :::8066                 :::*                    LISTEN      7115/java           
tcp6       0      0 192.168.200.11:9092     :::*                    LISTEN      6645/java           
tcp6       0      0 :::2181                 :::*                    LISTEN      6211/java           
~~~



# 五、部署集群应用系统



## 1、jar1和jar2两个节点上运行jar包

jar1和jar2两个节点上都运行4个jar包。

~~~powershell
[root@db1 ~]# cd gpmall-cluster/
[root@db1 gpmall-cluster]# nohup java -jar user-provider-0.0.1-SNAPSHOT.jar &
[1] 14140
[root@db1 gpmall-cluster]# nohup: ignoring input and appending output to ‘nohup.out’

[root@db1 gpmall-cluster]# nohup java -jar shopping-provider-0.0.1-SNAPSHOT.jar &
[2] 14152
[root@db1 gpmall-cluster]# nohup: ignoring input and appending output to ‘nohup.out’

[root@db1 gpmall-cluster]# nohup java -jar gpmall-shopping-0.0.1-SNAPSHOT.jar &
[3] 14201
[root@db1 gpmall-cluster]# nohup: ignoring input and appending output to ‘nohup.out’

[root@db1 gpmall-cluster]# nohup java -jar gpmall-user-0.0.1-SNAPSHOT.jar &
[4] 14242
[root@db1 gpmall-cluster]# nohup: ignoring input and appending output to ‘nohup.out’
~~~

检查

~~~powershell
[root@db1 gpmall-cluster]#  ps aux | grep java
.......................
root      14140 17.3  9.2 2566612 173168 pts/1  Sl   13:00   0:16 java -jar user-provider-0.0.1-SNAPSHOT.jar
root      14152 15.9  8.0 2551976 151112 pts/1  Sl   13:00   0:14 java -jar shopping-provider-0.0.1-SNAPSHOT.jar
root      14201 33.9 10.6 2548856 199928 pts/1  Sl   13:01   0:25 java -jar gpmall-shopping-0.0.1-SNAPSHOT.jar
root      14242 26.1  8.1 2541680 153016 pts/1  Sl   13:01   0:16 java -jar gpmall-user-0.0.1-SNAPSHOT.jar
root      14361  0.0  0.0 112644   948 pts/1    R+   13:02   0:00 grep --color=auto java
~~~



## 2、前端配置

~~~powershell
[root@mycat ~]# yum install nginx -y

[root@mycat ~]# rm -rf /usr/share/nginx/html/*
[root@mycat ~]# cp -rvf gpmall-cluster/dist/* /usr/share/nginx/html/
~~~

修改nginx配置文件

~~~powershell
[root@mycat ~]# vi /etc/nginx/conf.d/default.conf
//---------------------------------------------------------------------------在此处添加如下信息
upstream myuser {
    server 192.168.200.12:8082;
    server 192.168.200.13:8082;
    ip_hash;
    }

upstream myshopping {
    server 192.168.200.12:8081;
    server 192.168.200.13:8081;
    ip_hash;
    }

upstream mycashier {
    server 192.168.200.12:8083;
    server 192.168.200.13:8083;
    ip_hash;
    }
//-----------------------------------------------------------------------------

server {
    listen       80;
    server_name  localhost;

    #charset koi8-r;
    #access_log  /var/log/nginx/host.access.log  main;

    location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
    }
//---------------------------------------------------------------------------在此处添加如下信息
location /user {
    proxy_pass http://myuser;
    }

location /shopping {
    proxy_pass http://myshopping;
    }

location /cashier {
    proxy_pass http://mycashier;
    }
//-----------------------------------------------------------------------------
    #error_page  404              /404.html;

    # redirect server error pages to the static page /50x.html
    #
    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }

    # proxy the PHP scripts to Apache listening on 127.0.0.1:80
    #
    #location ~ \.php$ {
    #    proxy_pass   http://127.0.0.1;
    #}

    # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000
    #
    #location ~ \.php$ {
    #    root           html;
    #    fastcgi_pass   127.0.0.1:9000;
    #    fastcgi_index  index.php;
    #    fastcgi_param  SCRIPT_FILENAME  /scripts$fastcgi_script_name;
    #    include        fastcgi_params;
    #}

    # deny access to .htaccess files, if Apache's document root
    # concurs with nginx's one
    #
    #location ~ /\.ht {
    #    deny  all;
    #}
}
~~~



## 3、启动nginx

~~~powershell
[root@mycat ~]# systemctl start nginx
[root@mycat ~]# systemctl enable nginx
Created symlink from /etc/systemd/system/multi-user.target.wants/nginx.service to /usr/lib/systemd/system/nginx.service.

[root@mycat ~]# netstat -ntpl
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
tcp        0      0 0.0.0.0:6379            0.0.0.0:*               LISTEN      7217/redis-server * 
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      7395/nginx: master  		//看到nginx80端口即可
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1466/sshd           
tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      1972/master         
tcp        0      0 127.0.0.1:32000         0.0.0.0:*               LISTEN      7115/java           
tcp6       0      0 :::9066                 :::*                    LISTEN      7115/java           
tcp6       0      0 :::6379                 :::*                    LISTEN      7217/redis-server * 
tcp6       0      0 :::41835                :::*                    LISTEN      7115/java           
tcp6       0      0 :::60172                :::*                    LISTEN      7115/java           
tcp6       0      0 :::35663                :::*                    LISTEN      6645/java           
tcp6       0      0 192.168.200.11:3888     :::*                    LISTEN      6211/java           
tcp6       0      0 :::21                   :::*                    LISTEN      2875/vsftpd         
tcp6       0      0 :::22                   :::*                    LISTEN      1466/sshd           
tcp6       0      0 :::38232                :::*                    LISTEN      6211/java           
tcp6       0      0 ::1:25                  :::*                    LISTEN      1972/master         
tcp6       0      0 :::1984                 :::*                    LISTEN      7115/java           
tcp6       0      0 :::8066                 :::*                    LISTEN      7115/java           
tcp6       0      0 192.168.200.11:9092     :::*                    LISTEN      6645/java           
tcp6       0      0 :::2181                 :::*                    LISTEN      6211/java           
~~~



6、打开浏览器。浏览http://192.168.200.11,，即可浏览到1+X示例商城网站