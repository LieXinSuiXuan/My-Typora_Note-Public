2u=8.89cm


FusionComputer体系： 
两大组件： CNA VRM 
CNA：computer node agent 计算节点代理 
VRM：vir resource management 虚拟资源管理 
如何让物理主机具备虚拟化的能力？
直接在物理主机上安装一个操作系统(CNA.iso)
CNA.iso是一个华为定制的linux操作系统 

安装VRM，VRM也是一个定制版的linux

第一步：安装CNA
第二步：安装VRM 
有虚拟化节点的物理主机为CNA 

物理服务器的本地磁盘主要作用是承载操作系统

VRM部署方式： 
把VRM作为虚拟机部署在CNA节点上可以实现VRM和CNA共存
如果CNA主机数量小于50台，建议VRM虚拟化部署，把VRM部署在某几个CNA上 
如果CNA主机数量大于50台，建议VRM直接部署在物理机上 

单个虚拟机的规格不能超过CNA规格
VRM只做界面管理，下发指令，不跑业务 

如何搭建FusionComputer： 
1.在物理主机上部署CNA 
2.通过华为提供的工具FusionComputer installer 部署VRM 
对接CNA主机ip以及详细的信息 
部署完成后告知FC登录的IP地址 

现有服务器分类： 
三大类：
塔式服务器 
刀片式服务器
机架式服务器
1u的服务器=4.445cm
2u=8.89cm


mgmt服务器管理口

IPMI协议： 只能平台管理接口

不同厂家对IPMI(智能平台管理接口)提出的名字不一样，但是原理都是一样的 
DEEL： iDRA 远程访问控制C	
IBM： RSA 远程管理适配器
HP： iLO hp独有的服务器远程管理技术
huawei：v1 v2版本 iMANA(老版) v3版本 IBMC(新版)基板管理控制

host os 
guest os 

浮动IP： 
类似oracle的VIP，虚拟IP，作用于物理ip宕机，可以去动态连接到另一个ip

云计算里面的虚拟化技术： 
虚拟化的历史： 
云计算公司采用的虚拟化技术都是开源的免费的，方便二次开发，降低成本
两大阵营： XEN 、KVM 
华为虚拟化的产品：     
FusionComputer 6.1 操作系统之前使用的SUSE 
FusionComputer 6.3 操作系统之前只用的Euler OS 欧拉linux
FusionComputer 6.1 之前使用的XEN
FusionComputer 6.3 以后使用的KVM 

当时亚马逊使用的虚拟化技术是xen，当时没有开发kvm

kvm:基于内核的虚拟机

xen：单独把虚拟机内核拿出来，需要维护两套内核
FusionManager 管理虚拟资源 

虚拟化： 
CPU指令环： 
从内到外： ring0-ring1-ring2-ring3 
ring0跑操作系统内核： 内核态 
ring3跑应用程序：用户态
由内核态调用特权指令
用户态调用普通指令
如果应用程序需要涉及到特权 则向内核态申请 




虚拟化内容： 
IA阶段：
1.计算虚拟化
2.存储虚拟化
3.网络虚拟化

cna只能物理部署

可以只用集群创建虚拟机-负载均衡

容灾理念： 两地三中心
国内两个中心，国外一个中心 

内存复用技术： 
1.内存共享
2.内存气泡
3.内存置换（内存交换分区、swap分区） 
swap分区： 
冷数据被自动从内存中清除 刷到硬盘上  
这里面硬盘中的那一部分被刷进去的空间就是swap分区 

默认内存复用比可以达到150% 
内存复用比例可以自己调节 

guestOS 虚拟操作系统 
guestmachine 虚拟机 
hypervisor 虚拟化监控机  kvm xen hyper-v 
VMM虚拟化软件层 和hypervisor是一样的 










CPU虚拟化： 
目的： 为了解决内核在ring1层次上不能调用底层资源，因此诞生了cpu虚拟化技术 

内核态和用户态
ring0是内核态 
ring3是用户态 



缺点： 效率低下 

三种机制： 
完全虚拟化 
由VMM接管内核来代替内核做业务 
由VMM代替内核跑CPU指令，由VMM来骗内核来进行计算任务 
性能慢，VMM软件研发难度大 
虚拟机不知道自己是虚拟机 由VMM来欺骗 
虚拟机的用户态和物理机的用户态去调用不敏感的时候速度是一样的  
也就是说  用户态指令 虚拟机和物理机性能一样  
内核态 虚拟机新能没有物理机性能好  
虚拟机内核工作在ring1 
物理机内核工作在ring0  


半虚拟化
让虚拟机知道自己是虚拟机  
只能在开源的操作系统里面才能实现半虚
通过修改guestOS的操作系统内核
性能比全虚性能好很多 
windows不能做半虚
软件的兼容性和可移植性差

硬件辅助虚拟化 
vt-x amd-v技术 
在ring0的等级增加了两个模式：根模式和非根模式 
在cpu中集成了更多的指令，集成了敏感指令  
cpu硬件辅助虚拟化 	
VMM和hostOS都工作在ring0  但是VMM工作在根模式 hostOS工作在非根模式 
当不运行敏感指令的时候就不用VMM 直接用hostOS来调用 如果有敏感指令的话 就用VMM来调用指令 



内存虚拟化： 
内存映射 ，由VMM来欺骗内核来进行内核需要进行的内存索要 
内存正常工作需要从零开始并且连续  
适用于虚拟机和物理机 
VA应用程序使用内存
VM虚拟机上的应用不能直接去调用物理内存 
只能通过VMM来调用物理内存 

IO虚拟化 
输入输出虚拟化 
KVM虚拟化 
基于虚拟机内核的虚拟化 
现在的kvm=kvm.ko+qemu 
qemu用来做io虚拟化 
kvm用来做内存虚拟化 
和曾经的kvm不一样 
io虚拟化的性能高 
部署的方式非常轻量，部署快  



Xen虚拟化 
在装好xen的VMM后会自动创建一个特权虚拟机 
前端驱动捕捉到io请求后去调用特权虚拟机后端驱动
由特权虚拟机来回应io

FusionComputer结构 
VRM接管多个CNA 
多个CNA去通过交换机对接多个存储设备 
VRM主备部署，安全性更高，成本更高，可以不停业务 
主机数量小于50 可以主备可以单节点 可以虚拟机安装VRM 可以虚拟机安装VRM
主机数量大于50 采用主备部署 一般不适用虚拟化部署VRM 通常都是采用物理部署 
CNA和VRM采用同一个网段 保证能够通信 



搭建FC： 
安装CNA
进入跳板机 
输入BMC地址 
BMC接口进入web界面 
打开控制台 
启动虚拟控制台 
模拟出来一个光驱 
挂载ISO文件 虚拟介质 启动虚拟介质 添加一个ISO镜像文件  
设置为光驱启动 设置 第一个引导设备 虚拟CD 
重启服务器 
硬件驱动 看起来只有一块盘 但其实是有两块盘 两块盘做成了一个raid1 备份使用 
选择磁盘 定义swap分区大小 格式化 
网络配置 
配置eth0 网口 
配置带有vlan的地址 
系统主机名是CNA01 
时区北京 
密码huawei12#$ 
安装VRM
可以用工具安装VRM 也可以用安装CNA的方法去安装VRM
工具安装 选择安装VRM 可以批量安装 
告知VRM安装包路径  自动检测 
主备安装 
定义浮动ip 访问浮动ip的时候 不管主备哪一台挂了 还是可以通过浮动ip访问到另外一台 
仲裁ip 通过仲裁ip来裁定是否由一个主机挂掉了 然后拉活另一台机器的业务 不采用仲裁的话 可以写网关ip
选择主机ip 填写密码 
等待安装 


计算虚拟化： CPU虚拟化、内存虚拟化、IO虚拟化
全虚 特权解除的现象 由VMM代替内核  ring1 还是有一定的权限去调用  性能比较差 

现在的硬件辅助虚拟化 有root和非root层级 
敏感指令由root模式执行 
让VMM来执行 

存储虚拟化： 
存储分类： 
盘控一体： 
硬盘框和控制框是一起的
 硬盘框用于存资源 
 控制框去管理那些存入的资源 
盘控一体的存储 是由控制模块去控制的  

盘控分离：
硬盘框和控制框是分开的 
传统企业级存储 成本高 安全性高 一般用于政府银行的存储数据 

全闪存储： ssd存储 速度快 


硬盘框：2U 
硬盘模块：存储数据 
电源模块：主备  
级联模块：主备
系统插框： 单框，没有内容
硬盘框4U: 
系统插框 、电源模块 、硬盘模块 、级联模块、风扇模块 



控制框： 
电源模块 
硬盘模块
控制器模块 ： 独立控制框 A控和B控 
smartIO模块：用来控制硬盘 一般主备来提高安全性 

企业连存储模式： 
服务器连接两个交换机  
两台交换机连接控制框的A控和B控 交叉全连 保证业务不能断 
smartIO用来连接另外的控制框  用来提高计算能力  被成为scale-out模式 横向扩展计算能力 
exp接口用来提供容量扩展 

scale-up的时候遵守原则: 
1.两条路： 
2.一正一反 
3.exp-pri pri-exp
4.A控连A控 B控连B控 
保证了安全性的最大化 
A控正连 B控反连 或者A控反连B控正连 保证无论任何一个硬盘框挂了不影响别的硬盘框，保证任何一条线断了 不会影响任何一个存储的访问 
控制盘也有硬盘 但是容量有限 

硬盘介质 ： 
固态和机械 
接口类型： 
ATA接口 最开始的接口 速度最低 
IDE： 并行传输 一起传输 速度一样 速率低
SATA： 串行传输 一个一个传输 收尾相连 性能比ATA快 
SCSI： 小型计算机系统接口 USB等接口 
SAS： 
FC： 
SSD： 
NVME: 

在交换机连接到存储的那一部分网络 被称为存储网络 
SAN网络 存储区域网络  

机械硬盘的数据存入的原理就是在磁盘中写入磁性粒子 

SSD： 通过连接到SSD SSD控制器去计算数据 去存入到flash颗粒里面 然后直接去调用flash颗粒 当主机断电的时候 如果有数据没有存入到flash颗粒中 SSD有备用电源 可以让数据去存入ddr内存中 等待主机开机后 再存入到flash颗粒中

 SSD速率高 安全性高 但是不可找回数据  

机械硬盘速度慢是因为磁道在转 
机械硬盘有逻辑编址
现在逻辑编制已经被淘汰了 不需要采用多张盘片

存储可靠技术： 
传统RAID： 服务器里面采用传统RAID(独立硬盘冗余阵列)
RAID1： 两块盘 一块盘做数据写入 一块盘做数据备份 数据保护功能 
硬件RAID：
RAID卡来控制硬盘来做成RAID 这种为硬件RAID 
软件RAID：
用操作系统来做RAID的话 就是软件RAID  
RAID的单位就是条带 
条带：硬盘中单个或者连续的多个扇区可以组成一个条带 ，最小的条带就是一个扇区 大小是512字节 
分条： 同一个硬盘阵列的多个硬盘上相同位置的条带的集合 
分条深度： 条带的大小 
分条宽度：一个分条有几个条带 RAID中成员盘的个数 
RAID数据保护的方式： 
方法一：硬盘保存数据的副本 成本高 
方法二： 奇偶校验算法(XOR)
相同为假 相异为真 
假0 真1 
三个盘 两个数据盘 一个校验盘 
数据盘进行XOR运算 最后一块盘都会浪费存校验数据
RAID技术将多个单独的盘用不同的方式组成一个逻辑盘提高读写性能和数据的的安全性  根据不同的方式可以分为不同的级别


只允许坏一块盘 不能坏第二块 
RAID0： 提升性能 没有安全 采用了分条的方式  
n>=2 服务器里面可以用RAID卡来让一个盘做成RAID0 
如果RAID卡支持直通 可以直接使用 如果不支持 必须要做成RAID0才可以 (分条RAID) 没有舍弃空间 没有保护功能 如果坏盘 RAID0就坏了  空间利用率100% 没有数据保护 读写性能都好 

RAID1：镜像RAID 偶数块盘 1/n的利用率 通过拷贝来进行备份 空间利用率少 保护方式是副本 双写数据 如果坏盘 则需要拷贝副本盘到业务盘  写性能差 读性能好 负载均衡   

RAID3：奇偶校验 XOR运算 相同为假 相异为真 
假0 真1 
三个盘 两个数据盘 一个校验盘  数据盘进行XOR运算 最后一块盘都会浪费存校验数据  RAID技术将多个单独的盘用不同的方式组成一个逻辑盘提高读写性能和数据的的安全性  根据不同的方式可以分为不同的级别 盘数n>=3 读写性能较好 n-1个数据盘 一个校验盘 校验盘的寿命会很低 前面的任何一块盘进行修改 校验盘都要修改一次 校验盘可能成为性能瓶颈 坏盘就是降级状态 可以读写但是速率下降  

RAID5： 分布式奇偶校验 没有校验盘 但是有校验数据 每个盘都有校验数据  每个盘的条带都存校验数据 寿命都比较平均 n>=3 性能较好  

混合RAID：允许坏两个盘 不丢数据 
RAID6 两种校验算法的RAID类型 N+2个盘 最少5块盘 两个校验数据盘 三个数据盘 
常用的RAID6 
P+Q： P=异或算法 Q=特定值和数据做异或校验 
DP原理 双校验盘 都是用的异或校验 
坏的盘数超过两块才是失效状态 坏一块降级 两块也是降级 
RAID10 RAID1和RAID0组合方式 先做成RAID1 再把两个RAID1做成RAID0 双写了之后再做成一个RAID0 读写性能一般 可以坏特定的两盘 不能再坏RAID1中的备份盘 

RAID50  两个RAID5做成RAID0 可以坏两个盘 节省了 校验盘 保证了读写的性能 

RAID1安全性最高 安全和性能都要求的RAID5空间要求高  安全性极高RAID6  性能要求高RAID0 
相同性能盘才可以做成RAID 不同容量的盘去做RAID的时候 按最少容量的盘为RAID盘的主容量 会使容量浪费 


LUN虚拟化 
解决了性能和容量的冲突问题 把性能和存储都切开 然后重新组合 满足了性能和容量  
先划分RAID 然后再虚拟化 再划分完整的盘 服务器多了之后 性能还是跟不上 

RAID2.0： 现代存储采用 
软件逻辑对象
多块盘做成整体 做成RAID2.0 
硬盘域：  把硬盘分为域 来进行划分  一堆硬盘的组合 
热备盘：放一个备胎 来防止盘挂了之后重构数据浪费时间 不能当做主盘使用
热备策略： 跟热备盘的原理一样 但是是从每块硬盘中抽取空间来当做热备空间
硬盘域创建成功的时候 把硬盘全部切成了CK 放在硬盘域里面
CK：存储池内的磁盘空间切分成固定大小的物理空间 RAID的单位 为CK
CKG：不同的CK组合起来 组成的逻辑空间是CKG 是存储池从硬盘域上分配资源的最小单位 要在相同性能内 才可以组成CKG 而且CKG中的CK必须要来自不同的硬盘 

win： 
做完CIFS共享之后 在win主机中输入//逻辑业务口ip地址 可以搜索出来共享的文件夹 
在win中永久挂在磁盘 映射网络驱动器  

linux： 
showmount  -e 业务口ip  查看业务口的nfs访问列表 
mount -t nfs 挂载源 挂载目标 








openstack和openshift的区别 
openstack是IaaS层面的基础架构 
是将现有的服务器转化为云资源 
openshift是PaaS层面的平台 
运行在AWS等云服务之上 openshift是用于OpenStack上的基于云的服务 
openstack是服务器基础架构 
openshift是可以用作第三方的API服务 


RAID2.0 
硬盘框：特供存储 
控制框：提供计算 
exp-pri 正-反 A控-A控 b控-b控 
硬盘框中可以有不同种类的盘  
物理上是独立的 逻辑上是一个整体  

raid2.0原理：通过存储切成ck ckg  划分lun 利用lun来对接存储资源 

硬盘域只是一堆硬盘的组合   可以有不同性能的盘 

创建lun
extent吧存储池切成4M大小的块（默认4M）
4M大小的块组成lun
经常访问的数据为热数据 
不经常访问的数据为冷数据 
在lun中可以有不同的介质 可以有ssd和sics 
热数据存入SSD 冷数据存入sics来提高性能 
lun由extent组成  

硬盘域创建成功后ck被切好 
创建存储池组ckg 
extent在存储池中切 默认4M大小可以修改 

硬盘域--存储池--LUN--被利用  
thin lun 支配精简 又被成为精简lun 
闯将thin lun 会把extent切成grain 

精简lun：
thin lun 
按需分配 
创建速度快 
extent没有被直接使用 
使用thin lun的时候需要把extent切成grain  

普通lun： 
划分好后直接使用extent 可以直接向extent中写数据 

thin lun 可以实现超分配 可以在后期超预算的情况下通过扩容将超出的部分补上  

thin lun 在空间可用的状态使用完后 没有可以用的grain 但是在扩容后再划分grain后仍然可以继续使用 

thin lun的构成: CKG--Extent--grain--thin lun 
lun是人为概念 在底层中都以卷的形式出现在底层空间 
lun的本质是卷  

sun存储区域网络连接主机和存储的网络 一般采用主备部署 

DAS直连存储 距离小于等于2.5km
缺点： 距离限制 可能会产生资源孤岛、信息孤岛的现象 
外部DAS一般用于移动硬盘 
内部DAS一般用于自己电脑的硬盘 
DAS直连存储一般在服务器端被淘汰 而在PC端被采用

sun组网是传统企业最优秀的组网模式 中间交换机路由器等设备采用主备部署 来确保数据的安全性 保证业务不会断线 使用对象是LUN 
卷的本质是块设备存储 

所有的系统中只会存在一种文件系统 
NAS出生自带文件系统 网络附加存储 共享存储 

CIFS一般用于Windows系统 
NFS一般用于linux文件系统  
两者都是共享文件系统 
文件系统在存储上创建通过共享的方式去访问存储 
SAN：毛坯房 NFS：精装修 

LUN的映射： 
需要先创建文件系统
分为元数据区域和数据区域 
元数据区域上有索引、文件名 

如果需要使用LUN的话 必须创建映射关系 
通过索引用元数据跟数据连接起来 
元数据和磁盘的序号有一定的对应关系 

给主机的是块设备则用san的方式 通过san网络进行访问
主机来创建文件系统 

san的共享性差 读写快 
nas的共享性强 读写慢

FCsan： 中间的网络使用光纤 光纤交换机来连接 
IPsan： 中间的网络使用以太网线和以太网交换机来连接 
SAN可以用于备份 容灾 共享等 
容灾架构 可以作为SAN架构 双写 单读 来提高安全性 
融合存储 可以在存储中划分LUN 也可以划分文件系统 

存储池中的文件存储服务 直接划分为Grain 分给文件系统使用 也是按需分配

通过业务口访问数据 I/O口 
通过管理口去管理数据 

linux使用LUN iscsi发起实验步骤： 
1.配ip
2.生成iqn
打开/etc/iscsi/initiaforname.iscsi 
3.重启iscsi服务 
4.登录存储 
iscsiadm  -m discovery -t st -p 存储业务口ip地址 

iscsiadm -m node -c 
fdisk -l 扫盘 
service iscsi restart 重启服务 
扫出来的盘是裸盘 需要格式化文件系统 然后挂载 

NAS可以在一种文件系统中创建两种文件系统的类型 
即：cifs文件系统上可以有NFS和cifs类型的文件系统 但是NFS只读 cifs可读可写 

NAS共享： 
win下： 
1.配ip
2.创建文件系统 
3.共享 

模拟器estor01 可以做双活实验 把主机上的存储进行模拟 

模拟器使用步骤： 
使用VMware打开磁盘文件 创建虚拟机 创建出来的虚拟机可以通过web界面进行访问管理 
可以使用仅主机来配置网络 让虚拟机和主机通信 

VIMS 集群集文件系统  
CNA中创建的虚拟机会进行三次io转换 (由VMM进行的io转换)

fusionComputer虚拟化实验： 
1.登录搭建好的fusionComputer架构的web界面 
2.创建虚拟机模板 
 1) 创建虚拟机 
 2) 选择虚拟机创建的LUN 
 3) 
    1.本地挂载： 
    创建完成 打开电源 配置光驱 本地 挂载win10光驱 重启机器 
    2.输入共享 ip格式 ：\\xxx.xxx.xxx.x.x\iso\镜像名称 
     4）打开控制面板后 开机 制作模板 
     5）反键找到模板 按模板部署虚拟机 
     6）进入IE01 设置ipv4的地址 和主机通信 
     7）进入IE02 设置ipv4的地址 和主机通信 
     8）创建快照 配置 配置对象权限 添加 删除 更改网卡勾上为禁用 
     9）创建绑定磁盘 创建后分区挂载格式化保证可用 
     10）推迟删除时间 然后10分钟后开机 

 存储虚拟化和虚拟化存储 
 存储虚拟化是大的概念 而虚拟化存储是存储虚拟化的一个部分 
VIMS： 虚拟镜像管理系统 集群集文件系统 
虚拟机上的磁盘在物理机上只是文件 

虚拟化数据存储: 从物理的角度来看虚拟机磁盘是一个文件 但是在虚拟机的角度来看 磁盘就是磁盘 也就是说 在物理机中是file 但是在虚拟机中是block 其中需要进行三次io转换 

裸设备数据存储： 没有逻辑层 虚拟机和物理机的角度都是block 虚拟磁盘都是块设备 被一个虚拟机独占 没有共享能力 性能最好 不用做IO转换 只能做数据盘 不能做系统盘 

非虚拟化数据存储 ：FS中的资源 都是非虚拟化数据存储可以创建block给上层机器使用

FS只能作为非虚拟化存储 
存储虚拟化： 存储设备抽象为数据存储 
存储设备的概念更广 
虚拟化存储就是虚拟化数据存储 
数据存储是逻辑容器 类似文件系统 

关联主机就是关联主机的iqn 

链接克隆虚拟机 采用差分磁盘 相同数据内荣的数据在母盘 差异性数据放在子盘 采用这种方式创建虚拟机为创建连接克隆虚拟机 
使用链接克隆虚拟机让创建出来的虚拟机和原来的虚拟机之间的联系性更大 如果创建完整虚拟机的话 创建前后的两个虚拟机没有任何关系 相当于拷贝了一份一样的 多占用一倍内存 

虚拟化数据存储的高级特性 ： 
快照 
链接克隆 
内存扩容

非虚拟机化数据存储： 没有高级特性 
多数都是FusionStorage 
扫出FS池来做非虚拟化存储 
只需要做一次io转换 

FS 有一定的高级特性 但是没有虚拟化存储的高级特性多 
在FS层进行实验 让非虚拟化存储拥有高级特性 

网络虚拟化: 
创建Vlan
Vlan原理见数通 

网络资源虚拟化： 
OVS： 虚拟交换机（开源）
实现OVS的三种方式 
1.消耗CPU（OVS）
2.智能网卡（EVS）
3.物理交换机 

由OVS/EVS组成的集合叫DVS（分布式虚拟交换机）

CNA是需要上行链路 将物理网络和虚拟网络连接 

























































